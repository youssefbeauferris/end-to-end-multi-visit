{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def install(name):\n",
    "    subprocess.call(['pip', 'install', name])\n",
    "\n",
    "install('nibabel')\n",
    "install('scikit-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0353a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import nibabel as nib\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing our model\n",
    "MY_UTILS_PATH = \"../src/\"\n",
    "if not MY_UTILS_PATH in sys.path:\n",
    "    sys.path.append(MY_UTILS_PATH)\n",
    "import cs_models_sc as fsnet\n",
    "import tensorflow as tf\n",
    "# Importing callbacks and data augmentation utils\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import  Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c52eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "decay = 1e-7\n",
    "H,W = 512,512\n",
    "w1,w2 = 0,1\n",
    "model_name = \"../models/end-to-end-transfer-learning.hdf5\"\n",
    "save_path = '../data/predicted/10x-transfer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = np.loadtxt('../data/test.txt',dtype=str)\n",
    "ref_path = '../../../data/brain-cancer/'\n",
    "next_path = '../data/zero_filled_rec/10x/'\n",
    "ref_reg_path = '../data/reference_reg_10x/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathes to test data\n",
    "test_previous_files = [ref_reg_path + 'elastic_' + moving[:-4] + '_' + fixed for moving, fixed in zip(test_files[:,0],test_files[:,1])]\n",
    "test_follow_up_files = [ref_path + file for file in test_files[:,1]]\n",
    "#test_follow_up_files = [next_path + file for file in test_files[:,1]]\n",
    "test_reference_files = [ref_path + file for file in test_files[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate masks for k-space data of zero_filled reconstruction\n",
    "#there may be a better way of implementing this but will try this first\n",
    "# Loading sampling patterns. Notice that here we are using uncentred k-space\n",
    "var_sampling_mask = np.fft.fftshift(~np.load(\"../data/sampling_masks/R10_512x512_poisson_center_true_radius_40_r_2.66.npy\"),axes=(1,2))\n",
    "var_sampling_mask = np.concatenate((var_sampling_mask[:,:,:,np.newaxis],var_sampling_mask[:,:,:,np.newaxis]),axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 512, 512, 2)\n",
      "(152, 512, 512, 2)\n",
      "(152, 512, 512, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 23 layers into a model with 40 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e34dca6dc77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#model2 = Model(inputs=model.inputs, outputs=model.layers[-4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2359\u001b[0m               f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2360\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2361\u001b[0;31m           \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[0;31m# Perform any layer defined finalization of the layer state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    689\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                      ' layers.')\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 23 layers into a model with 40 layers."
     ]
    }
   ],
   "source": [
    "\n",
    "for ii, file in enumerate(test_follow_up_files):\n",
    "    img = nib.load(file)\n",
    "    zero_filled_rec = np.swapaxes(img.get_fdata(),0,2)\n",
    "    zero_filled_rec = zero_filled_rec / np.abs(zero_filled_rec).max()\n",
    "    #convert zero filled reconstruction to kspace and undersample\n",
    "    f = np.fft.fft2(zero_filled_rec)\n",
    "    zero_filled_kspace = np.zeros((*zero_filled_rec.shape,2))\n",
    "    zero_filled_kspace[:,:,:,0] = f.real\n",
    "    zero_filled_kspace[:,:,:,1] = f.imag\n",
    "    var_sampling_mask_test = np.tile(var_sampling_mask[0],(zero_filled_kspace.shape[0],1,1,1))\n",
    "    print(var_sampling_mask_test.shape)\n",
    "    zero_filled_kspace[:,var_sampling_mask[0]] = 0\n",
    "    print(zero_filled_kspace.shape)\n",
    "    \n",
    "    #load previous registered reconstruction\n",
    "    img2 = nib.load(test_previous_files[ii])\n",
    "    previous_rec = np.swapaxes(img2.get_fdata(),0,2)[...,np.newaxis]\n",
    "    previous_rec = previous_rec / np.abs(previous_rec).max()\n",
    "    print(previous_rec.shape)\n",
    "    #load our model\n",
    "    \n",
    "    model = fsnet.deep_cascade_flat_unrolled_end(\"iki\", H, W)\n",
    "    opt = Adam(learning_rate = lr,decay = decay)\n",
    "    model.compile(loss = ['mse','mse'],loss_weights=[w1,w2],optimizer=opt)\n",
    "    model.load_weights(model_name)\n",
    "    \n",
    "    #model2 = Model(inputs=model.inputs, outputs=model.layers[-4])\n",
    "    print(model.summary())\n",
    "    \n",
    "    pred = model.predict([zero_filled_kspace, previous_rec, var_sampling_mask_test])\n",
    "    \n",
    "    single = pred[0][:,:,:,0]\n",
    "    single = np.swapaxes(single, 0,2)\n",
    "    single_nifti = nib.Nifti1Image(single, img.affine)\n",
    "    \n",
    "    name = test_files[ii][1][:-4]\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    nib.save(single_nifti, save_path + '/' + name +'_single_visit.nii')\n",
    "    \n",
    "    multi = pred[1][:,:,:,0]\n",
    "    multi = np.swapaxes(multi, 0,2)\n",
    "    multi_nifti = nib.Nifti1Image(multi, img.affine)\n",
    "    name = test_files[ii][1][:-4]\n",
    "    nib.save(multi_nifti, save_path + '/' + name +'_multi_visit.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
